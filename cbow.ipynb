{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# from graphviz import Digraph\n",
    "\n",
    "\n",
    "class Matrix:\n",
    "\n",
    "    def __init__(self, val, _op='', _desc=()):\n",
    "        self.val = val\n",
    "        self.shape = val.shape\n",
    "        self.grad = np.zeros(self.shape)\n",
    "        self._backprop = lambda: None\n",
    "        self._prev = set(_desc)\n",
    "        self._op = _op\n",
    "\n",
    "    def __add__(self, other):\n",
    "        other = other if isinstance(other, Matrix) else Matrix(other)\n",
    "        res = Matrix(self.val + other.val, '+', (self, other))\n",
    "\n",
    "        def _backprop():\n",
    "            if self.shape == res.shape:\n",
    "                self.grad += res.grad\n",
    "            elif self.shape[0] == res.shape[0]:\n",
    "                self.grad += res.grad.sum(1, keepdims=True)\n",
    "            else:\n",
    "                self.grad += res.grad.sum()\n",
    "            # print(\"ADD Self:\", self.grad)\n",
    "            if other.shape == res.shape:\n",
    "                other.grad += res.grad\n",
    "            elif other.shape[0] == res.shape[0]:\n",
    "                other.grad += res.grad.sum(1, keepdims=True)\n",
    "            else:\n",
    "                other.grad += res.grad.sum()\n",
    "            # print(\"ADD -other:\", other.grad)\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        return self + other\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        other = other if isinstance(other, Matrix) else Matrix(other)\n",
    "        res = Matrix(self.val * other.val, '*', (self, other))\n",
    "\n",
    "        def _backprop():\n",
    "            if self.shape == res.shape:\n",
    "                self.grad += other.val * res.grad\n",
    "            elif self.shape[0] == res.shape[0]:\n",
    "                self.grad += (other.val * res.grad).sum(1, keepdims=True)\n",
    "            else:\n",
    "                self.grad += (other.val * res.grad).sum()\n",
    "            # print(\"Multiply Self:\", self.grad)\n",
    "\n",
    "            if other.shape == res.shape:\n",
    "                other.grad += self.val * res.grad\n",
    "            elif other.shape[0] == res.shape[0]:\n",
    "                other.grad += (self.val * res.grad).sum(1, keepdims=True)\n",
    "            else:\n",
    "                other.grad += (self.val * res.grad).sum()\n",
    "            # print(\"Multiply -other:\", other.grad)\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        return self * other\n",
    "\n",
    "    def __neg__(self):\n",
    "        return self * np.array([-1])\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        return self + (-other)\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        return other + (-self)\n",
    "\n",
    "    def __matmul__(self, other):\n",
    "        res = Matrix(self.val @ other.val, '@', (self, other))\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += res.grad @ other.val.T\n",
    "            other.grad += self.val.T @ res.grad\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def __pow__(self, power):\n",
    "        res = Matrix(self.val ** power, f'**{power}', (self,))\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += res.grad * (power * (self.val ** (power - 1)))\n",
    "            # print(\"Power backprop: \", self.grad)\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        return self * (other ** -1)\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        return other * (self ** -1)\n",
    "\n",
    "    def T(self):\n",
    "        res = Matrix(self.val.T, 'T', (self,))\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += res.grad.T\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def log(self):\n",
    "        res = Matrix(np.log(self.val), _op='log', _desc=(self,))\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += res.grad * (self.val ** -1)\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def softmax(self):\n",
    "        # print(\"softmax:\", self.shape)\n",
    "        shift = self.val - np.max(self.val, axis=1, keepdims=True)\n",
    "        exp_shift = np.exp(shift)\n",
    "        smax = exp_shift / np.sum(exp_shift, axis=1, keepdims=True)\n",
    "        res = Matrix(smax, 'smax', (self,))\n",
    "\n",
    "        def _backprop():\n",
    "            for i in range(smax.shape[0]):\n",
    "                grad_matrix = -np.outer(smax[i], smax[i]) + np.diag(smax[i])\n",
    "                self.grad[i] += (res.grad[i].reshape(1, -1) * grad_matrix).sum(1)\n",
    "\n",
    "        res._backprop = _backprop\n",
    "        return res\n",
    "\n",
    "    def cross_entropy(self, gold):\n",
    "        assert self.shape[0] == gold.shape[0], f\"number of outputs must be equal: {self.shape[0]} and {gold.shape[0]}\"\n",
    "        argmax_gold = np.argmax(gold.val, axis=1)\n",
    "        ceLoss = Matrix(np.array((-1.0 / self.shape[0]) * np.sum(np.log(self[np.arange(self.shape[0]), argmax_gold]))),\n",
    "                        _op='cen',\n",
    "                        _desc=(self, gold))\n",
    "\n",
    "        def _backprop():\n",
    "            self.grad += -1.0 * (gold.val / self.val) * ceLoss.grad\n",
    "            # no need to backprop for gold values\n",
    "\n",
    "        ceLoss._backprop = _backprop\n",
    "        return ceLoss\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.val[index]\n",
    "\n",
    "    def backprop(self):\n",
    "        order = []\n",
    "        visited = set()\n",
    "\n",
    "        def build_dependency_tree(node):\n",
    "            if node not in visited:\n",
    "                visited.add(node)\n",
    "                for prev_node in node._prev:\n",
    "                    build_dependency_tree(prev_node)\n",
    "                order.append(node)\n",
    "                # print(\"Step=\", v._op)\n",
    "\n",
    "        build_dependency_tree(self)\n",
    "\n",
    "        self.grad = np.ones(self.shape)\n",
    "        for v in reversed(order):\n",
    "            # print(v.val, \"Operator=\", v._op, v._backprop)\n",
    "            v._backprop()\n",
    "#             print(\"Grad:\", v.grad.sum()) \n",
    "        return order\n",
    "\n",
    "\n",
    "def zero_grad(order):\n",
    "    for v in order:\n",
    "        v.grad = np.zeros(v.grad.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"gutenberg\") \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(data):\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))   \n",
    "  punctuation = {'.','!', \"'\", \"''\", '(', ')', ',', '.', ':', ';', '?', '[', ']', '``', ' ', '_', '\"','*','%','$','&','+','-','/','\\\\','`','<','>','=','@'}\n",
    "\n",
    "  cleaned_data = []\n",
    "  unique_words = []\n",
    "  length = 0\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  for sent in data:\n",
    "    new_sent = []\n",
    "\n",
    "    for word in sent:\n",
    "      # removing any punctuations appended with the word\n",
    "      for w in word: \n",
    "          if(w in punctuation or w.isnumeric()):\n",
    "            word = word.replace(w,'')\n",
    "\n",
    "      # lowercasing the word\n",
    "      word = word.lower()\n",
    "      # removing stopwords and digits\n",
    "      if(word not in stop_words and word.isnumeric() == False and word !=''): \n",
    "          # lemmatize\n",
    "#           word = lemmatizer.lemmatize(word)  \n",
    "          # adding cleaned words to the sentence\n",
    "          new_sent.append(word)\n",
    "          # checking for unique words\n",
    "          if(word not in unique_words):\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # adding valid sentences to the data\n",
    "    if(len(new_sent) > 1):\n",
    "#         final_sent = []\n",
    "#         for word in new_sent:\n",
    "#             if word not in final_sent:\n",
    "#                 final_sent.append(word)\n",
    "        cleaned_data.append(new_sent)\n",
    "        length += len(new_sent)\n",
    "\n",
    "  print(\"Average length of sentences : \", length/(len(cleaned_data)))\n",
    "\n",
    "  return cleaned_data, unique_words\n",
    "\n",
    "  \n",
    "def create_vocab(words):\n",
    "    sorted_words = sorted(words) \n",
    "    vocab = {word: sorted_words.index(word) for word in sorted_words}\n",
    "    vocab['.'] = len(sorted_words)\n",
    "    return sorted_words, vocab\n",
    "\n",
    "\n",
    "def word_to_onehot(word, vocab):\n",
    "  onehot = np.zeros((len(vocab),))\n",
    "  # print(word)\n",
    "  onehot[vocab[word]] = 1.0\n",
    "  return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(sentences, vocab, window_size=1):\n",
    "    created_data = []\n",
    "    # for i in range(len(sentences)):\n",
    "    #     context_and_word = []\n",
    "    #     for j in range(i-window_size, i+window_size+1):\n",
    "    #         if j > 0 and j < len(sentences) and j != i:\n",
    "    #             context_and_word.append(word_to_onehot(sentences[i][j], vocab))\n",
    "    #     context_and_word.append(word_to_onehot(sentences[i], vocab))\n",
    "    #     data.append(context_and_word)\n",
    "    count = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = ['.']*window_size + sentence + ['.']*window_size \n",
    "#         print(sentence)\n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            context_and_word = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                if j != i:\n",
    "                    context_and_word.append(sentence[j])\n",
    "            context_and_word.append(sentence[i])\n",
    "            created_data.append(context_and_word)\n",
    "        count += 1\n",
    "        if count%5000 == 0:\n",
    "            print(f\"{count} sentences processed.\")\n",
    "\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of sentences :  11.38591095652952\n",
      "No. of samples:  89417\n",
      "A sample sentence:  ['emma', 'jane', 'austen']\n",
      "No. of unique words:  41361\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.corpus.gutenberg.sents()\n",
    "# print(\"No. of sentences: \",len(sentences))\n",
    "# print(\"A sample sentence: \",sentences[0])\n",
    "data, unique_words = preprocess_corpus(sentences)\n",
    "print(\"No. of samples: \",len(data))\n",
    "print(\"A sample sentence: \", data[0]) \n",
    "print(\"No. of unique words: \", len(unique_words))\n",
    "sorted_words, vocab = create_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 sentences processed.\n",
      "10000 sentences processed.\n",
      "15000 sentences processed.\n",
      "20000 sentences processed.\n",
      "25000 sentences processed.\n",
      "30000 sentences processed.\n",
      "35000 sentences processed.\n",
      "40000 sentences processed.\n",
      "45000 sentences processed.\n",
      "50000 sentences processed.\n",
      "55000 sentences processed.\n",
      "60000 sentences processed.\n",
      "65000 sentences processed.\n",
      "70000 sentences processed.\n",
      "75000 sentences processed.\n",
      "80000 sentences processed.\n",
      "85000 sentences processed.\n"
     ]
    }
   ],
   "source": [
    "final_data = create_data(data, vocab, window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1018094, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data), len(final_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def forward_pass(context, label_word, context_weight, softmax_weight, d, method=\"cbow\"):  # data in the form of list of context words\n",
    "    # context word: batch_size x |V|   weights: |V| x d\n",
    "    loss = 0\n",
    "    net = label_word @ context_weight\n",
    "    losses = []\n",
    "    for word in context:\n",
    "        out = (net @ softmax_weight).softmax()\n",
    "        context_loss = out.cross_entropy()\n",
    "        losses.append(context_loss)\n",
    "    loss = sum(losses) / len(context)\n",
    "    return loss\n",
    "\n",
    "\n",
    "# returns context and label\n",
    "def convert_to_onehot(batch):\n",
    "    context = []\n",
    "    label_word = np.zeros((len(batch), len(vocab)))\n",
    "    for i in range(len(batch[0])-1):\n",
    "        vectors = np.zeros((len(batch), len(vocab)))\n",
    "        for j in range(len(batch)):\n",
    "            onehot = word_to_onehot(batch[j][i], vocab)\n",
    "            vectors[j,:] = onehot\n",
    "        vectors = Matrix(vectors)\n",
    "        context.append(vectors)\n",
    "    for j in range(len(batch)):\n",
    "        onehot = word_to_onehot(batch[j][-1], vocab)\n",
    "        label_word[j,:] = onehot\n",
    "    label_word = Matrix(label_word)\n",
    "    \n",
    "    return context, label_word\n",
    "\n",
    "\n",
    "# data shape: N x C x |V|\n",
    "# data has first C-1 elements as context and last element as label word\n",
    "def train(data, weights, batch_size=4, num_epochs=100, lr=0.01, dim=100, method=\"cbow\"):\n",
    "    # context_weights = Matrix(np.random.randn(data.shape[2], dim))\n",
    "    # softmax_weights = Matrix(np.random.randn(dim, data.shape[2]))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        step = 0\n",
    "        losses = []\n",
    "        tolerance = 0\n",
    "        final_weights = [weights[0].val, weights[1].val]\n",
    "        time_i = time.time()\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i+batch_size-1]\n",
    "            context, label_word = convert_to_onehot(batch)\n",
    "            loss = forward_pass(context, label_word, weights[0], weights[1], dim)\n",
    "            order = loss.backprop()\n",
    "#             print(\"Weight:\", weights[0].shape, weights[0].grad.shape)\n",
    "            for weight in weights:\n",
    "                weight.val -= lr * weight.grad\n",
    "#             print(\"Weight:\", weights[0].shape)\n",
    "            zero_grad(order)\n",
    "            step += 1\n",
    "            if step % 100 == 0:\n",
    "                time_j = time.time()\n",
    "                print(f\"Epoch: {epoch+1} | Step: {step} | Loss: {loss.val:.5f} | Time: {time_j - time_i}\")\n",
    "        losses.append(loss.val)\n",
    "        if len(losses) > 10:\n",
    "            if losses[epoch] >= losses[epoch-1]:\n",
    "                tolerance += 1\n",
    "            else:\n",
    "                final_weights = [weights[0].val, weights[1].val]\n",
    "                tolerance = 0\n",
    "        if tolerance > 4:\n",
    "            break\n",
    "            \n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (4, 41362), (4, 41362))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context, label_word = convert_to_onehot(final_data[:4])\n",
    "len(context), context[0].shape, label_word.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 100 | Loss: 33.91208 | Time: 6242.003605604172\n",
      "Epoch: 1 | Step: 200 | Loss: 27.45938 | Time: 12520.953283309937\n"
     ]
    }
   ],
   "source": [
    "# from backpropagation import Matrix, zero_grad\n",
    "dim = 100\n",
    "context_weight = Matrix(np.random.randn(len(vocab), dim))\n",
    "softmax_weight = Matrix(np.random.randn(dim, len(vocab)))\n",
    "weights = [context_weight, softmax_weight]\n",
    "weights = train(final_data, weights, batch_size=4, num_epochs=30, lr=0.01, dim=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['.', 'jane', 'emma'],\n",
       " ['emma', 'austen', 'jane'],\n",
       " ['jane', '.', 'austen'],\n",
       " ['.', 'woodhouse', 'emma']]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f67010a807bd5d08e07febbad564d06fc913622566e721ece994d50519f80644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
