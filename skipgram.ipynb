{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"gutenberg\") \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(data):\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))   \n",
    "  punctuation = {'.','!', \"'\", \"''\", '(', ')', ',', '.', ':', ';', '?', '[', ']', '``', ' ', '_', '\"','*','%','$','&','+','-','/','\\\\','`','<','>','=','@'}\n",
    "\n",
    "  cleaned_data = []\n",
    "  unique_words = []\n",
    "  length = 0\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  for sent in data:\n",
    "    new_sent = []\n",
    "\n",
    "    for word in sent:\n",
    "      # removing any punctuations appended with the word\n",
    "      for w in word: \n",
    "          if(w in punctuation or w.isnumeric()):\n",
    "            word = word.replace(w,'')\n",
    "\n",
    "      # lowercasing the word\n",
    "      word = word.lower()\n",
    "      # removing stopwords and digits\n",
    "      if(word not in stop_words and word.isnumeric() == False and word !=''): \n",
    "          # lemmatize\n",
    "#           word = lemmatizer.lemmatize(word)  \n",
    "          # adding cleaned words to the sentence\n",
    "          new_sent.append(word)\n",
    "          # checking for unique words\n",
    "          if(word not in unique_words):\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # adding valid sentences to the data\n",
    "    if(len(new_sent) > 1):\n",
    "#         final_sent = []\n",
    "#         for word in new_sent:\n",
    "#             if word not in final_sent:\n",
    "#                 final_sent.append(word)\n",
    "        cleaned_data.append(new_sent)\n",
    "        length += len(new_sent)\n",
    "\n",
    "  print(\"Average length of sentences : \", length/(len(cleaned_data)))\n",
    "\n",
    "  return cleaned_data, unique_words\n",
    "\n",
    "  \n",
    "def create_vocab(words):\n",
    "    sorted_words = sorted(words) \n",
    "    vocab = {word: sorted_words.index(word) for word in sorted_words}\n",
    "    vocab['.'] = len(sorted_words)\n",
    "    return sorted_words, vocab\n",
    "\n",
    "\n",
    "def word_to_onehot(word, vocab):\n",
    "  onehot = np.zeros((len(vocab),))\n",
    "  # print(word)\n",
    "  onehot[vocab[word]] = 1.0\n",
    "  return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(sentences, vocab, window_size=1):\n",
    "    created_data = []\n",
    "    # for i in range(len(sentences)):\n",
    "    #     context_and_word = []\n",
    "    #     for j in range(i-window_size, i+window_size+1):\n",
    "    #         if j > 0 and j < len(sentences) and j != i:\n",
    "    #             context_and_word.append(word_to_onehot(sentences[i][j], vocab))\n",
    "    #     context_and_word.append(word_to_onehot(sentences[i], vocab))\n",
    "    #     data.append(context_and_word)\n",
    "    count = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = ['.']*window_size + sentence + ['.']*window_size \n",
    "#         print(sentence)\n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            context_and_word = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                if j != i:\n",
    "                    context_and_word.append(sentence[j])\n",
    "            context_and_word.append(sentence[i])\n",
    "            created_data.append(context_and_word)\n",
    "        count += 1\n",
    "        if count%5000 == 0:\n",
    "            print(f\"{count} sentences processed.\")\n",
    "\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of sentences :  11.38591095652952\n",
      "No. of samples:  89417\n",
      "A sample sentence:  ['emma', 'jane', 'austen']\n",
      "No. of unique words:  41361\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.corpus.gutenberg.sents()\n",
    "# print(\"No. of sentences: \",len(sentences))\n",
    "# print(\"A sample sentence: \",sentences[0])\n",
    "data, unique_words = preprocess_corpus(sentences)\n",
    "print(\"No. of samples: \",len(data))\n",
    "print(\"A sample sentence: \", data[0]) \n",
    "print(\"No. of unique words: \", len(unique_words))\n",
    "sorted_words, vocab = create_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 sentences processed.\n",
      "10000 sentences processed.\n",
      "15000 sentences processed.\n",
      "20000 sentences processed.\n",
      "25000 sentences processed.\n",
      "30000 sentences processed.\n",
      "35000 sentences processed.\n",
      "40000 sentences processed.\n",
      "45000 sentences processed.\n",
      "50000 sentences processed.\n",
      "55000 sentences processed.\n",
      "60000 sentences processed.\n",
      "65000 sentences processed.\n",
      "70000 sentences processed.\n",
      "75000 sentences processed.\n",
      "80000 sentences processed.\n",
      "85000 sentences processed.\n"
     ]
    }
   ],
   "source": [
    "final_data = create_data(data, vocab, window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(context, label_word, context_weight, softmax_weight, d, lr):  # data in the form of list of context words\n",
    "    # context word: batch_size x |V|   weights: |V| x d\n",
    "    net = label_word @ context_weight\n",
    "    losses = []\n",
    "    d_h = np.zeros(label_word.shape)\n",
    "    for word in context:\n",
    "        h = (net @ softmax_weight)\n",
    "        exp_out = np.exp(h)\n",
    "        out = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        argmax_label = np.argmax(word, axis=1)\n",
    "        context_loss = (-1.0 / out.shape[0]) * np.sum(np.log(out[np.arange(out.shape[0]), argmax_label]))\n",
    "        losses.append(context_loss)\n",
    "        \n",
    "        grad_matrix = np.zeros((out.shape[0], out.shape[1], out.shape[1]))\n",
    "        for i in range(out.shape[0]):\n",
    "            grad_matrix[i,:,:] = -np.outer(out[i,:], out[i,:]) + np.diag(out[i,:])\n",
    "        d_h += (-word / out) * np.sum(grad_matrix, axis=2)\n",
    "    \n",
    "    d_softmax_weight = net.T @ (d_h / len(context))\n",
    "    d_context_weight = label_word.T @ (d_h @ softmax_weight.T)\n",
    "    softmax_weight -= lr * d_softmax_weight\n",
    "    context_weight -= lr * d_context_weight\n",
    "        \n",
    "    loss = sum(losses) / len(context)\n",
    "    \n",
    "    return loss, context_weight, softmax_weight\n",
    "\n",
    "# returns context and label\n",
    "def convert_to_onehot(batch):\n",
    "    context = []\n",
    "    label_word = np.zeros((len(batch), len(vocab)))\n",
    "    for i in range(len(batch[0])-1):\n",
    "        vectors = np.zeros((len(batch), len(vocab)))\n",
    "        for j in range(len(batch)):\n",
    "            onehot = word_to_onehot(batch[j][i], vocab)\n",
    "            vectors[j,:] = onehot\n",
    "        context.append(vectors)\n",
    "    for j in range(len(batch)):\n",
    "        onehot = word_to_onehot(batch[j][-1], vocab)\n",
    "        label_word[j,:] = onehot\n",
    "    \n",
    "    return context, label_word\n",
    "\n",
    "\n",
    "# data shape: N x C x |V|\n",
    "# data has first C-1 elements as context and last element as label word\n",
    "def train(data, batch_size=4, num_epochs=100, lr=0.01, dim=100, method=\"cbow\"):\n",
    "    context_weights = np.random.randn(len(vocab), dim)\n",
    "    softmax_weights = np.random.randn(dim, len(vocab))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        step = 0\n",
    "        losses = []\n",
    "        tolerance = 0\n",
    "        final_weights = []\n",
    "        time_i = time.time()\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch = data[i:i+batch_size-1]\n",
    "            context, label_word = convert_to_onehot(batch)\n",
    "            loss, context_weights, softmax_weights = forward_pass(context, label_word, context_weights, softmax_weights, dim, lr)\n",
    "            step += 1\n",
    "            if step % 1 == 0:\n",
    "                time_j = time.time()\n",
    "                print(f\"Epoch: {epoch+1} | Step: {step} | Loss: {loss:.5f} | Time: {(time_j - time_i) / 60}\")\n",
    "        losses.append(loss)\n",
    "        if len(losses) > 10:\n",
    "            if losses[epoch] >= losses[epoch-1]:\n",
    "                tolerance += 1\n",
    "            else:\n",
    "                final_weights = [context_weights, softmax_weights]\n",
    "                tolerance = 0\n",
    "        if tolerance > 3:\n",
    "            break\n",
    "            \n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 1 | Loss: 41.01988 | Time: 168.87529516220093\n",
      "Epoch: 1 | Step: 2 | Loss: 39.95448 | Time: 336.5445432662964\n",
      "Epoch: 2 | Step: 1 | Loss: 41.01988 | Time: 168.00080370903015\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_142293/1050972927.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfinal_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_142293/1029582537.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, batch_size, num_epochs, lr, dim, method)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_to_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mforward_pass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msoftmax_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_142293/1029582537.py\u001b[0m in \u001b[0;36mforward_pass\u001b[0;34m(context, label_word, context_weight, softmax_weight, d, lr)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mgrad_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mgrad_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0md_h\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mword\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_weights = train(final_data[:8], batch_size=2, num_epochs=5, lr=0.05, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "f67010a807bd5d08e07febbad564d06fc913622566e721ece994d50519f80644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
