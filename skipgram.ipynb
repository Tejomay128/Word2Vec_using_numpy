{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"gutenberg\") \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Obtaining data from NLTK</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(data):\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))   \n",
    "  punctuation = {'.','!', \"'\", \"''\", '(', ')', ',', '.', ':', ';', '?', '[', ']', '``', ' ', '_', '\"','*','%','$','&','+','-','/','\\\\','`','<','>','=','@'}\n",
    "\n",
    "  cleaned_data = []\n",
    "  unique_words = []\n",
    "  length = 0\n",
    "\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  for sent in data:\n",
    "    new_sent = []\n",
    "\n",
    "    for word in sent:\n",
    "      # removing any punctuations appended with the word\n",
    "      for w in word: \n",
    "          if(w in punctuation or w.isnumeric()):\n",
    "            word = word.replace(w,'')\n",
    "\n",
    "      # lowercasing the word\n",
    "      word = word.lower()\n",
    "      # removing stopwords and digits\n",
    "      if(word not in stop_words and word.isnumeric() == False and word !=''): \n",
    "          # lemmatize\n",
    "#           word = lemmatizer.lemmatize(word)  \n",
    "          # adding cleaned words to the sentence\n",
    "          new_sent.append(word)\n",
    "          # checking for unique words\n",
    "          if(word not in unique_words):\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # adding valid sentences to the data\n",
    "    if(len(new_sent) > 1):\n",
    "#         final_sent = []\n",
    "#         for word in new_sent:\n",
    "#             if word not in final_sent:\n",
    "#                 final_sent.append(word)\n",
    "        cleaned_data.append(new_sent)\n",
    "        length += len(new_sent)\n",
    "\n",
    "  print(\"Average length of sentences : \", length/(len(cleaned_data)))\n",
    "\n",
    "  return cleaned_data, unique_words\n",
    "\n",
    "  \n",
    "def create_vocab(words):\n",
    "    sorted_words = sorted(words) \n",
    "    vocab = {word: sorted_words.index(word) for word in sorted_words}\n",
    "    vocab['.'] = len(sorted_words)\n",
    "    return sorted_words, vocab\n",
    "\n",
    "\n",
    "def word_to_onehot(word, vocab):\n",
    "  onehot = np.zeros((len(vocab),))\n",
    "  # print(word)\n",
    "  onehot[vocab[word]] = 1.0\n",
    "  return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data instances in the form of [context_1, context_2,....,context_n, label word]\n",
    "def create_data(sentences, window_size=1):\n",
    "    created_data = []\n",
    "    count = 0\n",
    "    for sentence in sentences:\n",
    "        sentence = ['.']*window_size + sentence + ['.']*window_size \n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            context_and_word = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                if j != i:\n",
    "                    context_and_word.append(sentence[j])\n",
    "            context_and_word.append(sentence[i])\n",
    "            created_data.append(context_and_word)\n",
    "        count += 1\n",
    "        if count%10000 == 0:\n",
    "            print(f\"{count} sentences processed.\")\n",
    "\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length of sentences :  11.38591095652952\n",
      "No. of samples:  89417\n",
      "A sample sentence:  ['emma', 'jane', 'austen']\n",
      "No. of unique words:  41361\n"
     ]
    }
   ],
   "source": [
    "sentences = nltk.corpus.gutenberg.sents()\n",
    "# print(\"No. of sentences: \",len(sentences))\n",
    "# print(\"A sample sentence: \",sentences[0])\n",
    "data, unique_words = preprocess_corpus(sentences)\n",
    "print(\"No. of samples: \",len(data))\n",
    "print(\"A sample sentence: \", data[0]) \n",
    "print(\"No. of unique words: \", len(unique_words))\n",
    "sorted_words, vocab = create_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 sentences processed.\n",
      "10000 sentences processed.\n",
      "15000 sentences processed.\n",
      "20000 sentences processed.\n",
      "25000 sentences processed.\n",
      "30000 sentences processed.\n",
      "35000 sentences processed.\n",
      "40000 sentences processed.\n",
      "45000 sentences processed.\n",
      "50000 sentences processed.\n",
      "55000 sentences processed.\n",
      "60000 sentences processed.\n",
      "65000 sentences processed.\n",
      "70000 sentences processed.\n",
      "75000 sentences processed.\n",
      "80000 sentences processed.\n",
      "85000 sentences processed.\n"
     ]
    }
   ],
   "source": [
    "final_data = create_data(data, window_size=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#implements forward and backward pass given batch and weight, returns updated weights\n",
    "def single_pass(context, label_word, context_weight, softmax_weight, lr):  # data in the form of list of context words\n",
    "    # context word: batch_size x |V|   context_weight: |V| x d    softmax_weight: d x |V|\n",
    "    net = label_word @ context_weight\n",
    "    losses = []\n",
    "    d_net = np.zeros(net.shape)\n",
    "    d_softmax_weight = np.zeros(softmax_weight.shape)\n",
    "    #forward pass\n",
    "    for word in context:\n",
    "        h = (net @ softmax_weight)\n",
    "        exp_out = np.exp(h - h.max(1, keepdims=True))\n",
    "        out = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        argmax_label = np.argmax(word, axis=1)\n",
    "        context_loss = (-1.0 / out.shape[0]) * np.sum(np.log(1e-16 + out[np.arange(out.shape[0]), argmax_label]))\n",
    "        losses.append(context_loss)\n",
    "    \n",
    "    #backward pass\n",
    "        interm = -word * (1.0 - out)\n",
    "        d_softmax_weight += net.T @ interm\n",
    "        d_net = interm @ softmax_weight.T\n",
    "    \n",
    "    d_softmax_weight /= len(context)\n",
    "    d_net /= len(context)\n",
    "    d_context_weight = label_word.T @ d_net\n",
    "\n",
    "    softmax_weight -= lr * d_softmax_weight\n",
    "    context_weight -= lr * d_context_weight\n",
    "    loss = sum(losses) / len(context)\n",
    "    \n",
    "    return loss, context_weight, softmax_weight\n",
    "\n",
    "# returns context and label in the form of onehot vectors\n",
    "def convert_to_onehot(batch):\n",
    "    context = []\n",
    "    label_word = np.zeros((len(batch), len(vocab)))\n",
    "    for i in range(len(batch[0])-1):\n",
    "        vectors = np.zeros((len(batch), len(vocab)))\n",
    "        for j in range(len(batch)):\n",
    "            onehot = word_to_onehot(batch[j][i], vocab)\n",
    "            vectors[j,:] = onehot\n",
    "        context.append(vectors)\n",
    "    for j in range(len(batch)):\n",
    "        onehot = word_to_onehot(batch[j][-1], vocab)\n",
    "        label_word[j,:] = onehot\n",
    "    \n",
    "    return context, label_word\n",
    "\n",
    "\n",
    "# data shape: N x C x |V|\n",
    "# data has first C-1 elements as context and last element as label word\n",
    "def train(data, batch_size=4, num_epochs=100, lr=0.01, dim=100, method=\"cbow\"):\n",
    "    context_weights = np.random.randn(len(vocab), dim)\n",
    "    softmax_weights = np.random.randn(dim, len(vocab))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        step = 0\n",
    "        losses = []\n",
    "        tolerance = 0\n",
    "        final_weights = [context_weights, softmax_weights]\n",
    "        time_i = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "\n",
    "            batch = data[i:i+batch_size]\n",
    "            context, label_word = convert_to_onehot(batch)\n",
    "            loss, context_weights, softmax_weights = single_pass(context, label_word, context_weights, softmax_weights, lr)\n",
    "            epoch_loss += loss\n",
    "            step += 1\n",
    "            if step % 50 == 0:\n",
    "                time_j = time.time()\n",
    "                print(f\"Epoch: {epoch+1} | Step: {step} | Loss: {loss:.5f}\")\n",
    "        time_j = time.time()\n",
    "        epoch_loss /= step\n",
    "        print(f\"Epoch {1} finished | Epoch loss: {epoch_loss} | Time: {(time_j - time_i) / 60 : .5f} mins\")\n",
    "        losses.append(epoch_loss)\n",
    "        if len(losses) > 10:\n",
    "            if losses[epoch] >= losses[epoch-1]:\n",
    "                tolerance += 1\n",
    "            else:\n",
    "                final_weights = [context_weights, softmax_weights]\n",
    "                tolerance = 0\n",
    "            if tolerance > 3:\n",
    "                break\n",
    "            \n",
    "    return final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 50 | Loss: 33.19232\n",
      "Epoch: 1 | Step: 100 | Loss: 31.65288\n",
      "Epoch: 1 | Step: 150 | Loss: 34.51743\n",
      "Epoch: 1 | Step: 200 | Loss: 34.91714\n",
      "Epoch: 1 | Step: 250 | Loss: 35.08074\n",
      "Epoch 1 finished | Epoch loss: 33.511906310587555 | Time:  26.70667 mins\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m final_weights \u001b[39m=\u001b[39m train(final_data[:\u001b[39m256000\u001b[39;49m], batch_size\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m, num_epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, lr\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[77], line 66\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(data, batch_size, num_epochs, lr, dim, method)\u001b[0m\n\u001b[1;32m     64\u001b[0m batch \u001b[39m=\u001b[39m data[i:i\u001b[39m+\u001b[39mbatch_size]\n\u001b[1;32m     65\u001b[0m context, label_word \u001b[39m=\u001b[39m convert_to_onehot(batch)\n\u001b[0;32m---> 66\u001b[0m loss, context_weights, softmax_weights \u001b[39m=\u001b[39m single_pass(context, label_word, context_weights, softmax_weights, lr)\n\u001b[1;32m     67\u001b[0m epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m     68\u001b[0m step \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[77], line 18\u001b[0m, in \u001b[0;36msingle_pass\u001b[0;34m(context, label_word, context_weight, softmax_weight, lr)\u001b[0m\n\u001b[1;32m     15\u001b[0m     losses\u001b[39m.\u001b[39mappend(context_loss)\n\u001b[1;32m     17\u001b[0m \u001b[39m#backward pass\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m     interm \u001b[39m=\u001b[39m \u001b[39m-\u001b[39;49mword \u001b[39m*\u001b[39m (\u001b[39m1.0\u001b[39m \u001b[39m-\u001b[39m out)\n\u001b[1;32m     19\u001b[0m     d_softmax_weight \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m net\u001b[39m.\u001b[39mT \u001b[39m@\u001b[39m interm\n\u001b[1;32m     20\u001b[0m     d_net \u001b[39m=\u001b[39m interm \u001b[39m@\u001b[39m softmax_weight\u001b[39m.\u001b[39mT\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "final_weights = train(final_data[:256000], batch_size=1024, num_epochs=30, lr=0.01, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f67010a807bd5d08e07febbad564d06fc913622566e721ece994d50519f80644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
