{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d5c158",
   "metadata": {},
   "source": [
    "# Importing libraries and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47fd756e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home1/tejomay/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0496a5d3",
   "metadata": {},
   "source": [
    "# Gutenberg corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce9ffe16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Her', 'father', 'composed', 'himself', 'to', 'sleep', 'after', 'dinner', ',', 'as', 'usual', ',', 'and', 'she', 'had', 'then', 'only', 'to', 'sit', 'and', 'think', 'of', 'what', 'she', 'had', 'lost', '.']\n"
     ]
    }
   ],
   "source": [
    "g_sentences = nltk.corpus.gutenberg.sents() #Sentences in Gutenberg corpus\n",
    "print(g_sentences[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e42c09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98552"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_sentences) #Number of sentences in Gutenberg corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b5d6c9",
   "metadata": {},
   "source": [
    "# Wikipedia corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d11ecfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wikipedia (/home1/tejomay/.cache/huggingface/datasets/wikipedia/20220301.en/2.0.0/aa542ed919df55cc5d3347f42dd4521d05ca68751f50dbc32bae2a7f1e167559)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 6458670\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki = load_dataset(\"wikipedia\" , \"20220301.en\" , split = \"train\")\n",
    "wiki"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66237846",
   "metadata": {},
   "source": [
    "# Listing Analogies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c429b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = []\n",
    "\n",
    "with open('Analogy_dataset.txt','r') as file:\n",
    "    for line in file.readlines():\n",
    "        line = line.strip('\\n')\n",
    "        text.append(line)\n",
    "        \n",
    "analogy_dataset = text[:-1]\n",
    "\n",
    "groups = []\n",
    "word_list = []\n",
    "\n",
    "for analogy in analogy_dataset:\n",
    "    words = analogy.split()\n",
    "    words = [word.capitalize() for word in words]\n",
    "    word_list = word_list + words\n",
    "    group = words[:2]\n",
    "    if group not in groups:\n",
    "        groups.append(group)\n",
    "    group = words[2:]\n",
    "    if group not in groups:\n",
    "        groups.append(group)\n",
    "        \n",
    "word_list = list(set(word_list))#Retaining only the distinct words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4779edfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Athens', 'Greece'], ['Tehran', 'Iran'], ['London', 'England'], ['Madrid', 'Spain'], ['Bangkok', 'Thailand'], ['Islamabad', 'Pakistan'], ['Beijing', 'China'], ['Tokyo', 'Japan'], ['Berlin', 'Germany'], ['Rome', 'Italy'], ['Cairo', 'Egypt'], ['Oslo', 'Norway'], ['Ottawa', 'Canada'], ['Kabul', 'Afghanistan'], ['Canberra', 'Australia'], ['Doha', 'Qatar'], ['Hanoi', 'Vietnam'], ['Stockholm', 'Sweden'], ['Budapest', 'Hungary'], ['Kathmandu', 'Nepal'], ['Dhaka', 'Bangladesh'], ['Khartoum', 'Sudan'], ['Kiev', 'Ukraine'], ['Lisbon', 'Portugal'], ['Quito', 'Ecuador'], ['Riga', 'Latvia'], ['India', 'Asia'], ['Paris', 'Europe'], ['China', 'Asia'], ['Greece', 'Europe'], ['Nigeria', 'Africa'], ['France', 'Europe'], ['Kenya', 'Africa'], ['Netherlands', 'Europe'], ['Mumbai', 'Asia'], ['Nairobi', 'Africa'], ['Maharastra', 'Mumbai'], ['Karnataka', 'Bengaluru'], ['Telengana', 'Hyderabad'], ['Odisha', 'Bhubaneswar'], ['Gujurat', 'Gandhinagar'], ['Bihar', 'Patna'], ['Chhattisgarh', 'Raipur'], ['Assam', 'Dispur'], ['Goa', 'Panaji'], ['Rajasthan', 'Jaipur'], ['Jharkhand', 'Ranchi'], ['Punjab', 'Chandigarh'], ['Tripura', 'Agartala'], ['Kerala', 'Thiruvananthapuram'], ['India', 'Delhi'], ['Serbia', 'Belgrade'], ['Spain', 'Spanish'], ['Egypt', 'Arabic'], ['Syria', 'Arabic'], ['Australia', 'English'], ['Mouse', 'Squeak'], ['Elephant', 'Trumpet'], ['Algeria', 'Dinar'], ['Usa', 'Dollar'], ['Argentina', 'Peso'], ['Russia', 'Ruble'], ['Armenia', 'Dram'], ['Iran', 'Rial'], ['Brazil', 'Real'], ['Sweden', 'Krona'], ['Europe', 'Euro'], ['Japan', 'Yen'], ['India', 'Rupee'], ['Denmark', 'Krone'], ['Nigeria', 'Naira'], ['Switzerland', 'Swiss'], ['Thailand', 'Thai'], ['India', 'Indian'], ['Sweden', 'Swedish'], ['Netherlands', 'Dutch'], ['Korea', 'Korean'], ['Russia', 'Russian'], ['Germany', 'German'], ['Portugal', 'Portuguese'], ['Slovakia', 'Slovakian'], ['Poland', 'Polish'], ['Italy', 'Italian'], ['Norway', 'Norwegian'], ['Mexico', 'Mexican'], ['Japan', 'Japanese'], ['Australia', 'Australian'], ['Ireland', 'Irish'], ['Croatia', 'Croatian'], ['France', 'French'], ['Denmark', 'Danish']]\n"
     ]
    }
   ],
   "source": [
    "print(groups) #Here each sub-list is a analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c42eae7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(groups) #Number of unique analogies present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "497b07aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Ukraine', 'Australian', 'Gandhinagar', 'Greece', 'Asia', 'Bhubaneswar', 'German', 'Swedish', 'Euro', 'Ranchi', 'Arabic', 'Trumpet', 'Iran', 'Tehran', 'Delhi', 'England', 'Riga', 'Ruble', 'Belgrade', 'Thai', 'Argentina', 'Slovakia', 'Polish', 'Kiev', 'Australia', 'Madrid', 'Gujurat', 'Ecuador', 'Portuguese', 'Paris', 'Bengaluru', 'Telengana', 'Cairo', 'Tripura', 'Naira', 'China', 'Raipur', 'Irish', 'Oslo', 'Squeak', 'Kathmandu', 'Quito', 'Nairobi', 'Dinar', 'Berlin', 'Mouse', 'Spain', 'Ireland', 'Dutch', 'Jharkhand', 'Portugal', 'Kerala', 'Islamabad', 'Agartala', 'Italy', 'Canada', 'Dispur', 'Stockholm', 'Egypt', 'Punjab', 'Russian', 'Croatia', 'Kabul', 'Africa', 'Afghanistan', 'Germany', 'Sudan', 'Bihar', 'Dhaka', 'Hungary', 'Armenia', 'Panaji', 'Nepal', 'Korea', 'Chhattisgarh', 'India', 'Brazil', 'Indian', 'London', 'Thailand', 'Thiruvananthapuram', 'Jaipur', 'Danish', 'French', 'Vietnam', 'English', 'Krona', 'Dollar', 'Dram', 'Canberra', 'Spanish', 'Elephant', 'France', 'Goa', 'Chandigarh', 'Hanoi', 'Italian', 'Swiss', 'Syria', 'Karnataka', 'Japan', 'Nigeria', 'Japanese', 'Ottawa', 'Netherlands', 'Assam', 'Serbia', 'Budapest', 'Khartoum', 'Latvia', 'Mumbai', 'Switzerland', 'Athens', 'Mexico', 'Pakistan', 'Rupee', 'Rial', 'Slovakian', 'Tokyo', 'Hyderabad', 'Peso', 'Bangkok', 'Usa', 'Rome', 'Krone', 'Russia', 'Croatian', 'Sweden', 'Europe', 'Lisbon', 'Qatar', 'Denmark', 'Beijing', 'Doha', 'Real', 'Odisha', 'Maharastra', 'Poland', 'Patna', 'Korean', 'Yen', 'Bangladesh', 'Mexican', 'Kenya', 'Norway', 'Rajasthan', 'Norwegian']\n"
     ]
    }
   ],
   "source": [
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88217877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list) #Number of unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dc4c88",
   "metadata": {},
   "source": [
    "# Extracting Wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38d2df35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Algeria', 'Asia', 'Afghanistan', 'Arabic', 'Athens', 'Berlin', 'Brazil', 'Bangladesh', 'China', 'Croatia', 'Cairo', 'Dollar', 'Dutch', 'Danish', 'Europe', 'Elephant', 'England', 'English', 'Ecuador', 'Euro', 'French', 'Germany', 'German', 'Greece', 'Hungary', 'Italy', 'India', 'Italian', 'Iran', 'Indian', 'Irish', 'Japan', 'Japanese', 'Krone', 'Korea', 'Kabul', 'Karnataka', 'Kathmandu', 'Khartoum', 'Latvia', 'London', 'Lisbon', 'Mouse', 'Mumbai', 'Netherlands', 'Norway', 'Nigeria', 'Nairobi', 'Ottawa', 'Oslo', 'Poland', 'Paris', 'Portugal', 'Pakistan', 'Punjab', 'Portuguese', 'Polish', 'Russia', 'Russian', 'Rome', 'Riga', 'Rajasthan', 'Spain', 'Stockholm', 'Switzerland', 'Slovakia', 'Spanish', 'Sudan', 'Serbia', 'Tokyo', 'Thailand', 'Trumpet', 'Ukraine', 'Budapest', 'Squeak', 'Hyderabad', 'Delhi', 'Chhattisgarh', 'Islamabad', 'Canberra', 'Belgrade', 'Thiruvananthapuram', 'Bangkok', 'Dhaka', 'Hanoi', 'Tehran', 'Norwegian', 'Denmark', 'Thai', 'Korean', 'Ireland', 'Jaipur', 'Nepal', 'Peso', 'Assam', 'Kenya', 'Jharkhand', 'Vietnam', 'Gandhinagar', 'Quito', 'Chandigarh', 'Rupee', 'Dinar', 'Odisha', 'Croatian', 'Krona', 'Panaji', 'Ranchi', 'Tripura', 'Bihar', 'Agartala', 'Rial', 'Dram', 'Raipur', 'Dispur', 'Bhubaneswar', 'Swiss', 'Ruble', 'Mexico', 'Kerala', 'Australia', 'Canada', 'Sweden', 'Africa', 'France', 'Syria', 'Egypt', 'Qatar', 'Armenia', 'Mexican', 'Swedish', 'Beijing', 'Patna', 'Argentina', 'Doha', 'Real', 'Goa', 'Madrid']\n"
     ]
    }
   ],
   "source": [
    "words_with_wiki_articles = [title for title in wiki['title'] if title in word_list]\n",
    "print(words_with_wiki_articles) #All words for which wiki article is present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8911bc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words_with_wiki_articles) #So 138(out of 148) words have wiki articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1e0f38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australian', 'Kiev', 'Gujurat', 'Bengaluru', 'Telengana', 'Naira', 'Slovakian', 'Usa', 'Maharastra', 'Yen']\n"
     ]
    }
   ],
   "source": [
    "words_without_wiki_articles = [word for word in word_list if word not in words_with_wiki_articles]\n",
    "print(words_without_wiki_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2f1478b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Australian', 'Kiev', 'Bengaluru', 'Naira', 'Slovakian', 'Usa', 'Yen']\n"
     ]
    }
   ],
   "source": [
    "#Spelling errors\n",
    "\n",
    "#1.Telengana\n",
    "word_list.remove('Telengana')\n",
    "word_list.append('Telangana')\n",
    "groups.remove(['Telengana', 'Hyderabad'])\n",
    "groups.append(['Telangana', 'Hyderabad'])\n",
    "\n",
    "#2. Gujurat\n",
    "word_list.remove('Gujurat')\n",
    "word_list.append('Gujarat')\n",
    "groups.remove(['Gujurat', 'Gandhinagar'])\n",
    "groups.append(['Gujarat', 'Gandhinagar'])\n",
    "\n",
    "#3. Maharastra\n",
    "word_list.remove('Maharastra')\n",
    "word_list.append('Maharashtra')\n",
    "groups.remove(['Maharastra', 'Mumbai'])\n",
    "groups.append(['Maharashtra', 'Mumbai'])\n",
    "\n",
    "#Linking\n",
    "\n",
    "#1.Bengaluru\n",
    "word_list.append('Bangalore')\n",
    "groups.remove(['Karnataka', 'Bengaluru'])\n",
    "groups.append(['Karnataka', 'Bangalore'])\n",
    "groups.append(['Bangalore','Bengaluru'])\n",
    "\n",
    "#2.Kiev\n",
    "word_list.append('Kyiv')\n",
    "groups.remove(['Kiev', 'Ukraine'])\n",
    "groups.append(['Kyiv', 'Ukraine'])\n",
    "\n",
    "words_with_wiki_articles = [title for title in wiki['title'] if title in word_list]\n",
    "words_without_wiki_articles = [word for word in word_list if word not in words_with_wiki_articles]\n",
    "print(words_without_wiki_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "490afacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = {title : i for i, title in enumerate(wiki['title']) if title in word_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc1072b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indices)#Indices for words which have wiki article. So finally for 7 words, there is no wiki article present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37d7bb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index(title):\n",
    "    for i, t in enumerate(wiki['title']):\n",
    "        if t ==title:\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15d56254",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices['Usa'] = find_index('United States')\n",
    "indices['Dollar'] = find_index('United States dollar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17010e03",
   "metadata": {},
   "source": [
    "# Collecting relevant sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f5de953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_data = {}\n",
    "\n",
    "for group in groups:\n",
    "    paragraphs = []\n",
    "    for word in group:\n",
    "        if word in indices.keys():\n",
    "            paragraphs += wiki[indices[word]]['text'].replace('( ;  ;  (pl.) )','').split('\\n\\n')\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        sents = sent_tokenize(para)\n",
    "        sentences += sents\n",
    "        \n",
    "    relevant_sentences = []\n",
    "    \n",
    "    for sent in sentences:\n",
    "        tokenized_sent = word_tokenize(sent)\n",
    "        present = True\n",
    "        for word in group:\n",
    "            if word not in tokenized_sent:\n",
    "                present = False\n",
    "        if present == True:\n",
    "            relevant_sentences.append(tokenized_sent)\n",
    "            \n",
    "    analogy_data[tuple(group)] = relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ddd16b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mumbai', 'Asia']  :  4\n",
      "['Syria', 'Arabic']  :  4\n",
      "['Australia', 'English']  :  2\n",
      "['Mouse', 'Squeak']  :  0\n",
      "['Elephant', 'Trumpet']  :  0\n",
      "['Algeria', 'Dinar']  :  0\n",
      "['Usa', 'Dollar']  :  0\n",
      "['Argentina', 'Peso']  :  0\n",
      "['Russia', 'Ruble']  :  1\n",
      "['Armenia', 'Dram']  :  0\n",
      "['Iran', 'Rial']  :  0\n",
      "['Brazil', 'Real']  :  1\n",
      "['Sweden', 'Krona']  :  0\n",
      "['Europe', 'Euro']  :  1\n",
      "['Japan', 'Yen']  :  0\n",
      "['India', 'Rupee']  :  1\n",
      "['Denmark', 'Krone']  :  0\n",
      "['Nigeria', 'Naira']  :  0\n",
      "['Slovakia', 'Slovakian']  :  0\n"
     ]
    }
   ],
   "source": [
    "sparse_data_groups = []\n",
    "for group in groups:\n",
    "    if len(analogy_data[tuple(group)]) < 5:\n",
    "        print(group,' : ',len(analogy_data[tuple(group)]))\n",
    "        sparse_data_groups.append(tuple(group))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79c3b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for group in sparse_data_groups:\n",
    "    relevant_sentences = []\n",
    "    \n",
    "    for item in group:\n",
    "        if item in indices.keys():\n",
    "            count = 0\n",
    "\n",
    "            paragraphs= wiki[indices[item]]['text'].replace('( ;  ;  (pl.) )','').split('\\n\\n')\n",
    "\n",
    "            sentences = []\n",
    "\n",
    "            for para in paragraphs:\n",
    "                sents = sent_tokenize(para)\n",
    "                sentences += sents\n",
    "\n",
    "            for sent in sentences:\n",
    "                tokenized_sent = word_tokenize(sent)\n",
    "                if item in tokenized_sent:\n",
    "                    count += 1\n",
    "                    relevant_sentences.append(tokenized_sent)\n",
    "                if count == 5:break\n",
    "            \n",
    "    analogy_data[tuple(group)] += relevant_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d6b6d91c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Athens', 'Greece']  :  27, ['Tehran', 'Iran']  :  38, ['London', 'England']  :  36, ['Madrid', 'Spain']  :  39, ['Bangkok', 'Thailand']  :  27, ['Islamabad', 'Pakistan']  :  24, ['Beijing', 'China']  :  54, ['Tokyo', 'Japan']  :  26, ['Berlin', 'Germany']  :  49, ['Rome', 'Italy']  :  44, ['Cairo', 'Egypt']  :  42, ['Oslo', 'Norway']  :  44, ['Ottawa', 'Canada']  :  27, ['Kabul', 'Afghanistan']  :  30, ['Canberra', 'Australia']  :  12, ['Doha', 'Qatar']  :  35, ['Hanoi', 'Vietnam']  :  38, ['Stockholm', 'Sweden']  :  33, ['Budapest', 'Hungary']  :  27, ['Kathmandu', 'Nepal']  :  59, ['Dhaka', 'Bangladesh']  :  34, ['Khartoum', 'Sudan']  :  28, ['Lisbon', 'Portugal']  :  49, ['Quito', 'Ecuador']  :  32, ['Riga', 'Latvia']  :  38, ['India', 'Asia']  :  18, ['Paris', 'Europe']  :  14, ['China', 'Asia']  :  16, ['Greece', 'Europe']  :  15, ['Nigeria', 'Africa']  :  29, ['France', 'Europe']  :  44, ['Kenya', 'Africa']  :  23, ['Netherlands', 'Europe']  :  15, ['Mumbai', 'Asia']  :  14, ['Nairobi', 'Africa']  :  18, ['Odisha', 'Bhubaneswar']  :  23, ['Bihar', 'Patna']  :  41, ['Chhattisgarh', 'Raipur']  :  14, ['Assam', 'Dispur']  :  6, ['Goa', 'Panaji']  :  25, ['Rajasthan', 'Jaipur']  :  21, ['Jharkhand', 'Ranchi']  :  13, ['Punjab', 'Chandigarh']  :  13, ['Tripura', 'Agartala']  :  20, ['Kerala', 'Thiruvananthapuram']  :  31, ['India', 'Delhi']  :  63, ['Serbia', 'Belgrade']  :  48, ['Spain', 'Spanish']  :  30, ['Egypt', 'Arabic']  :  9, ['Syria', 'Arabic']  :  14, ['Australia', 'English']  :  12, ['Mouse', 'Squeak']  :  7, ['Elephant', 'Trumpet']  :  10, ['Algeria', 'Dinar']  :  5, ['Usa', 'Dollar']  :  5, ['Argentina', 'Peso']  :  6, ['Russia', 'Ruble']  :  8, ['Armenia', 'Dram']  :  9, ['Iran', 'Rial']  :  7, ['Brazil', 'Real']  :  11, ['Sweden', 'Krona']  :  7, ['Europe', 'Euro']  :  9, ['Japan', 'Yen']  :  5, ['India', 'Rupee']  :  11, ['Denmark', 'Krone']  :  9, ['Nigeria', 'Naira']  :  5, ['Switzerland', 'Swiss']  :  22, ['Thailand', 'Thai']  :  26, ['India', 'Indian']  :  28, ['Sweden', 'Swedish']  :  36, ['Netherlands', 'Dutch']  :  46, ['Korea', 'Korean']  :  25, ['Russia', 'Russian']  :  26, ['Germany', 'German']  :  20, ['Portugal', 'Portuguese']  :  50, ['Slovakia', 'Slovakian']  :  5, ['Poland', 'Polish']  :  42, ['Italy', 'Italian']  :  56, ['Norway', 'Norwegian']  :  35, ['Mexico', 'Mexican']  :  27, ['Japan', 'Japanese']  :  19, ['Australia', 'Australian']  :  20, ['Ireland', 'Irish']  :  37, ['Croatia', 'Croatian']  :  18, ['France', 'French']  :  63, ['Denmark', 'Danish']  :  23, ['Telangana', 'Hyderabad']  :  30, ['Gujarat', 'Gandhinagar']  :  11, ['Maharashtra', 'Mumbai']  :  24, ['Karnataka', 'Bangalore']  :  30, ['Bangalore', 'Bengaluru']  :  7, ['Kyiv', 'Ukraine']  :  35, "
     ]
    }
   ],
   "source": [
    "for group in groups:\n",
    "    print(group,' : ',len(analogy_data[tuple(group)]),end=', ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a3557dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Mumbai', 'Asia')  :  14, ('Syria', 'Arabic')  :  14, ('Australia', 'English')  :  12, ('Mouse', 'Squeak')  :  7, ('Elephant', 'Trumpet')  :  10, ('Algeria', 'Dinar')  :  5, ('Usa', 'Dollar')  :  5, ('Argentina', 'Peso')  :  6, ('Russia', 'Ruble')  :  8, ('Armenia', 'Dram')  :  9, ('Iran', 'Rial')  :  7, ('Brazil', 'Real')  :  11, ('Sweden', 'Krona')  :  7, ('Europe', 'Euro')  :  9, ('Japan', 'Yen')  :  5, ('India', 'Rupee')  :  11, ('Denmark', 'Krone')  :  9, ('Nigeria', 'Naira')  :  5, ('Slovakia', 'Slovakian')  :  5, "
     ]
    }
   ],
   "source": [
    "for group in sparse_data_groups:\n",
    "    print(group,' : ',len(analogy_data[tuple(group)]),end=', ')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e90c198",
   "metadata": {},
   "source": [
    "for group in groups:\n",
    "    print(group,':\\n\\n')\n",
    "    print(analogy_data[tuple(group)])\n",
    "    print('\\n-------------------------------------------------------------------------------------\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1ce4d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The', 'United', 'States', 'dollar', '(', 'symbol', ':', ';', 'code', ':', 'USD', ';', 'also', 'abbreviated', 'US', '$', 'or', 'U.S.', 'Dollar', ',', 'to', 'distinguish', 'it', 'from', 'other', 'dollar-denominated', 'currencies', ';', 'referred', 'to', 'as', 'the', 'dollar', ',', 'U.S.', 'dollar', ',', 'American', 'dollar', ',', 'or', 'colloquially', 'buck', ')', 'is', 'the', 'official', 'currency', 'of', 'the', 'United', 'States', 'and', 'its', 'territories', '.'], ['``', 'Dollar', \"''\", 'is', 'one', 'of', 'the', 'first', 'words', 'of', 'Section', '9', ',', 'in', 'which', 'the', 'term', 'refers', 'to', 'the', 'Spanish', 'milled', 'dollar', ',', 'or', 'the', 'coin', 'worth', 'eight', 'Spanish', 'reales', '.'], ['Dollar', 'sign'], ['The', 'last', 'coins', 'to', 'be', 'converted', 'to', 'profiles', 'of', 'historic', 'Americans', 'were', 'the', 'dime', '(', '1946', ')', 'and', 'the', 'Dollar', '(', '1971', ')', '.'], ['The', 'U.S.', 'Dollar', 'Index', 'is', 'an', 'important', 'indicator', 'of', 'the', 'dollar', \"'s\", 'strength', 'or', 'weakness', 'versus', 'a', 'basket', 'of', 'six', 'foreign', 'currencies', '.']]\n"
     ]
    }
   ],
   "source": [
    "print(analogy_data[('Usa', 'Dollar')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a2e80fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "k = 20\n",
    "\n",
    "for group in groups:\n",
    "    if len(analogy_data[tuple(group)])<20:\n",
    "        data += analogy_data[tuple(group)]\n",
    "    else:\n",
    "        data += analogy_data[tuple(group)][:20]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26360fb7",
   "metadata": {},
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2d509e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1517\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a95da942",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['You', 'do', 'not', 'think', 'I', 'could', 'mean', '_you_', ',', 'or', 'suppose', 'Mr', '.', 'Knightley', 'to', 'mean', '_you_', '.']\n"
     ]
    }
   ],
   "source": [
    "print(g_sentences[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "717f2e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Islamabad', '(', ';', ')', 'is', 'the', 'capital', 'city', 'of', 'Pakistan', ',', 'and', 'is', 'administered', 'by', 'the', 'Pakistani', 'federal', 'government', 'as', 'part', 'of', 'the', 'Islamabad', 'Capital', 'Territory', '.']\n"
     ]
    }
   ],
   "source": [
    "print(data[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "474308a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_data = g_sentences + data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48c1f1f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100069"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(concatenated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e63f4106",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'danger', ',', 'however', ',', 'was', 'at', 'present', 'so', 'unperceived', ',', 'that', 'they', 'did', 'not', 'by', 'any', 'means', 'rank', 'as', 'misfortunes', 'with', 'her', '.']\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_data[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f5bcfd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Kyiv', 'is', 'the', 'undisputed', 'center', 'of', 'business', 'and', 'commerce', 'of', 'Ukraine', 'and', 'home', 'to', 'the', 'country', \"'s\", 'largest', 'companies', ',', 'such', 'as', 'Naftogaz', 'Ukrainy', ',', 'Energorynok', 'and', 'Kyivstar', '.']\n"
     ]
    }
   ],
   "source": [
    "print(concatenated_data[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a69b580a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "index_list = list(range(len(concatenated_data)))\n",
    "random.shuffle(index_list)\n",
    "shuffled_dataset = [concatenated_data[i] for i in index_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4dfc7e0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100069"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(shuffled_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fa767d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'days', 'of', 'joyful', 'security', 'were', 'immediately', 'followed', 'by', 'the', 'over', '-', 'throw', 'of', 'every', 'thing', '.']\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_dataset[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a980e295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['16', ':', '23', 'The', 'heart', 'of', 'the', 'wise', 'teacheth', 'his', 'mouth', ',', 'and', 'addeth', 'learning', 'to', 'his', 'lips', '.']\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_dataset[-1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "72eb7cd4",
   "metadata": {},
   "source": [
    "<h2>CBOW and skipgram</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9fbee42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f15cc56e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(data):\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))   \n",
    "  punctuation = {'.','!', \"'\", \"''\", '(', ')', ',', '.', ':', ';', '?', '[', ']', '``', ' ', '_', '\"','*','%','$','&','+','-','/','\\\\','`','<','>','=','@'}\n",
    "\n",
    "  cleaned_data = []\n",
    "  unique_words = []\n",
    "  length = 0\n",
    "\n",
    "  # lemmatizer = WordNetLemmatizer()\n",
    "  count = 0\n",
    "  for sent in data:\n",
    "    count += 1\n",
    "    new_sent = []\n",
    "\n",
    "    for word in sent:\n",
    "      # removing any punctuations appended with the word\n",
    "      for w in word: \n",
    "          if(w in punctuation or w.isnumeric()):\n",
    "            word = word.replace(w,'')\n",
    "\n",
    "      # lowercasing the word\n",
    "      word = word.lower()\n",
    "      # removing stopwords and digits\n",
    "      if(word not in stop_words and word.isnumeric() == False and word !=''): \n",
    "          # lemmatize\n",
    "#           word = lemmatizer.lemmatize(word)  \n",
    "          # adding cleaned words to the sentence\n",
    "          new_sent.append(word)\n",
    "          # checking for unique words\n",
    "          if(word not in unique_words):\n",
    "            unique_words.append(word)\n",
    "\n",
    "    # adding valid sentences to the data\n",
    "    if(len(new_sent) > 1):\n",
    "#         final_sent = []\n",
    "#         for word in new_sent:\n",
    "#             if word not in final_sent:\n",
    "#                 final_sent.append(word)\n",
    "        cleaned_data.append(new_sent)\n",
    "        length += len(new_sent)\n",
    "    if count % 10000 == 0:\n",
    "        print(f\"{count} sentences processed.\")\n",
    "\n",
    "  print(\"Average length of sentences : \", length/(len(cleaned_data)))\n",
    "\n",
    "  return cleaned_data, unique_words\n",
    "\n",
    "  \n",
    "def create_vocab(words):\n",
    "    sorted_words = sorted(words) \n",
    "    vocab = {word: sorted_words.index(word) for word in sorted_words}\n",
    "    return sorted_words, vocab\n",
    "\n",
    "\n",
    "def word_to_onehot(word, vocab):\n",
    "  onehot = np.zeros((len(vocab),))\n",
    "  # print(word)\n",
    "  onehot[vocab[word]] = 1.0\n",
    "  return onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97df3261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data instances in the form of [context_1, context_2,....,context_n, label word]\n",
    "def create_data(sentences, window_size=1):\n",
    "    created_data = []\n",
    "    for sentence in sentences:\n",
    "        sentence = ['.']*window_size + sentence + ['.']*window_size \n",
    "        for i in range(window_size, len(sentence) - window_size):\n",
    "            context_and_word = []\n",
    "            for j in range(i-window_size, i+window_size+1):\n",
    "                if j != i:\n",
    "                    context_and_word.append(sentence[j])\n",
    "            context_and_word.append(sentence[i])\n",
    "            created_data.append(context_and_word)\n",
    "\n",
    "    return created_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cda7427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 sentences processed.\n",
      "20000 sentences processed.\n",
      "30000 sentences processed.\n",
      "40000 sentences processed.\n",
      "50000 sentences processed.\n",
      "60000 sentences processed.\n",
      "70000 sentences processed.\n",
      "80000 sentences processed.\n",
      "90000 sentences processed.\n",
      "100000 sentences processed.\n",
      "Average length of sentences :  11.482679745749664\n",
      "No. of samples:  90934\n",
      "A sample sentence:  ['thou', 'shalt', 'make', 'seven', 'lamps', 'thereof', 'shall', 'light', 'lamps', 'thereof', 'may', 'give', 'light']\n",
      "No. of unique words:  44661\n"
     ]
    }
   ],
   "source": [
    "data, unique_words = preprocess_corpus(shuffled_dataset)\n",
    "unique_words.append('.')\n",
    "print(\"No. of samples: \",len(data))\n",
    "print(\"A sample sentence: \", data[0]) \n",
    "print(\"No. of unique words: \", len(unique_words))\n",
    "sorted_words, vocab = create_vocab(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2acc7a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = create_data(data, window_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "917af079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1044166"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(final_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6bb7f54a",
   "metadata": {},
   "source": [
    "<h3>Training</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "28833d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implements forward and backward pass given batch and weight, returns updated weights\n",
    "def single_pass(context, label_word, context_weight, softmax_weight, lr):  # data in the form of list of context words\n",
    "    # context word: batch_size x |V|   context_weight: |V| x d    softmax_weight: d x |V|\n",
    "    net = label_word @ context_weight\n",
    "    losses = []\n",
    "    d_net = np.zeros(net.shape)\n",
    "    d_softmax_weight = np.zeros(softmax_weight.shape)\n",
    "    #forward pass\n",
    "    for word in context:\n",
    "        h = (net @ softmax_weight)\n",
    "        exp_out = np.exp(h - h.max(1, keepdims=True))\n",
    "        out = exp_out / np.sum(exp_out, axis=1, keepdims=True)\n",
    "        argmax_label = np.argmax(word, axis=1)\n",
    "        context_loss = (-1.0 / out.shape[0]) * np.sum(np.log(1e-16 + out[np.arange(out.shape[0]), argmax_label]))\n",
    "        losses.append(context_loss)\n",
    "    \n",
    "    #backward pass\n",
    "        interm = word - out\n",
    "        d_softmax_weight += net.T @ interm\n",
    "        d_net = interm @ softmax_weight.T\n",
    "    \n",
    "    d_softmax_weight /= len(context)\n",
    "    d_net /= len(context)\n",
    "    d_context_weight = label_word.T @ d_net\n",
    "\n",
    "    softmax_weight -= lr * d_softmax_weight\n",
    "    context_weight -= lr * d_context_weight\n",
    "    loss = sum(losses) / len(context)\n",
    "    \n",
    "    return loss, context_weight, softmax_weight\n",
    "\n",
    "# returns context and label in the form of onehot vectors\n",
    "def convert_to_onehot(batch):\n",
    "    context = []\n",
    "    label_word = np.zeros((len(batch), len(vocab)))\n",
    "    for i in range(len(batch[0])-1):\n",
    "        vectors = np.zeros((len(batch), len(vocab)))\n",
    "        for j in range(len(batch)):\n",
    "            onehot = word_to_onehot(batch[j][i], vocab)\n",
    "            vectors[j,:] = onehot\n",
    "        context.append(vectors)\n",
    "    for j in range(len(batch)):\n",
    "        onehot = word_to_onehot(batch[j][-1], vocab)\n",
    "        label_word[j,:] = onehot\n",
    "    \n",
    "    return context, label_word\n",
    "\n",
    "\n",
    "def nearest_word(vector, context_weights):\n",
    "    scores = (context_weights / np.sqrt(np.sum(np.square(context_weights),\n",
    "            axis=1, \n",
    "            keepdims=True))) @ (vector.reshape(-1,1) / np.sqrt(np.sum(np.square(vector))))\n",
    "    max_score_index = np.argmax(scores, axis=0)\n",
    "    return unique_words[max_score_index[0]]\n",
    "\n",
    "\n",
    "def validation(context_weights):\n",
    "    FILE = 'Validation.txt'\n",
    "    count = 0\n",
    "    correct = 0\n",
    "    with open(FILE, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            ignore = False\n",
    "            count += 1\n",
    "            words = line.strip('\\n').split()\n",
    "            for word in words:\n",
    "                if word not in vocab.keys():\n",
    "                    ignore = True\n",
    "            if not ignore:\n",
    "                vectors = [context_weights[vocab[word],:] for word in words[:len(words)-1]]\n",
    "                final_vector = vectors[0] - vectors[1] + vectors[2]\n",
    "                ans = nearest_word(final_vector, context_weights)\n",
    "                if ans == words[-1]:\n",
    "                    correct += 1\n",
    "    return correct / count\n",
    "\n",
    "\n",
    "# data shape: N x C x |V|\n",
    "# data has first C-1 elements as context and last element as label word\n",
    "def train(data, batch_size=4, num_epochs=100, lr=0.01, dim=100, method=\"cbow\"):\n",
    "    context_weights = np.random.randn(len(vocab), dim)\n",
    "    softmax_weights = np.random.randn(dim, len(vocab))\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        step = 0\n",
    "        losses = []\n",
    "        tolerance = 0\n",
    "        final_weights = [context_weights, softmax_weights]\n",
    "        time_i = time.time()\n",
    "        epoch_loss = 0.0\n",
    "        for i in range(0, len(data), batch_size):\n",
    "\n",
    "            if (i+batch_size) > len(data):\n",
    "                batch = data[i:]\n",
    "            else:\n",
    "                batch = data[i:i+batch_size]\n",
    "            context, label_word = convert_to_onehot(batch)\n",
    "            loss, context_weights, softmax_weights = single_pass(context, label_word, context_weights, softmax_weights, lr)\n",
    "            epoch_loss += loss\n",
    "            step += 1\n",
    "            if step % 50 == 0:\n",
    "                time_j = time.time()\n",
    "                print(f\"Epoch: {epoch+1} | Step: {step} | Loss: {loss:.4f}\")\n",
    "        time_j = time.time()\n",
    "        epoch_loss /= step\n",
    "        val_acc = validation(context_weights)\n",
    "        print(f\"Epoch {epoch+1} finished | Epoch loss: {epoch_loss: .4f} | Val acc: {val_acc:.4f} | Time: {(time_j - time_i) / 60 : .5f} mins\")\n",
    "        losses.append(epoch_loss)\n",
    "        if len(losses) > 7:\n",
    "            if losses[epoch] >= losses[epoch-1]:\n",
    "                tolerance += 1\n",
    "            else:\n",
    "                final_weights = [context_weights, softmax_weights]\n",
    "                tolerance = 0\n",
    "            if tolerance > 3 or val_acc > 0.92:\n",
    "                break\n",
    "            \n",
    "    return final_weights\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e69ddc1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Step: 50 | Loss: 33.5744\n"
     ]
    }
   ],
   "source": [
    "final_weights = train(final_data, batch_size=2048, num_epochs=40, lr=0.008, dim=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae956c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_weights = np.random.randn(len(vocab), 100)\n",
    "val_acc = validation(context_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4fa82c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "f67010a807bd5d08e07febbad564d06fc913622566e721ece994d50519f80644"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
